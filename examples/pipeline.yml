pipeline_name: S3 Object Processor
pipeline_description: |
  Comprehensive example demonstrating all major samstacks features including
  security-focused output masking. This pipeline deploys an S3 bucket with 
  a Lambda processor triggered by object uploads via SQS notifications.
  
  Features demonstrated:
  - Security-focused output masking for production deployments
  - Stack dependencies and output passing
  - Pipeline inputs with typed parameters
  - Conditional deployment logic
  - Post-deployment automation scripts

pipeline_settings:
  stack_name_prefix: samstacks-demo-${{ inputs.environment }}-
  default_region: ${{ env.AWS_REGION || 'us-west-2' }}
  
  # Enable comprehensive output masking for security
  # NOTE: When enabled=true, all categories below are enabled by default.
  # You can omit the 'categories' section entirely for the same effect.
  output_masking:
    enabled: true
    categories:
      # Mask AWS account IDs (12-digit numbers in ARNs, URLs, etc.)
      account_ids: true
      # Mask API Gateway URLs and Lambda Function URLs
      api_endpoints: true
      # Mask database endpoints (RDS, ElastiCache, etc.)
      database_endpoints: true
      # Mask load balancer DNS names
      load_balancer_dns: true
      # Mask CloudFront distribution domains
      cloudfront_domains: true
      # Mask S3 bucket website endpoints
      s3_bucket_domains: true
      # Mask IP addresses (IPv4 and IPv6)
      ip_addresses: true
    # Custom patterns for application-specific masking
    custom_patterns:
      - pattern: "secret-[a-zA-Z0-9]+"
        replacement: "secret-***"
        description: "Mask secret tokens"
      - pattern: "key-[0-9]+"
        replacement: "key-***"
        description: "Mask API keys"
  
  # Global SAM configuration
  default_sam_config:
    version: 0.1
    default:
      deploy:
        parameters:
          capabilities: CAPABILITY_IAM
          confirm_changeset: false
          resolve_s3: true
          tags: |
            Environment=${{ inputs.environment }}
            ManagedBy=samstacks
            Project=S3ObjectProcessor
  
  inputs:
    environment:
      type: string
      default: ${{ env.ENVIRONMENT || 'dev' }}
      description: "Target deployment environment (dev, staging, prod)"
    
    enable_storage_stack:
      type: boolean
      default: true
      description: "Whether to deploy the S3 storage stack"

stacks:
  - id: processor
    name: Lambda Processor Stack
    description: Lambda function with SQS queue for processing S3 object upload notifications
    dir: stacks/processor/
    params:
      Environment: ${{ inputs.environment }}
      EnableDetailedLogging: ${{ inputs.environment != 'prod' }}
      EnableXrayTracing: ${{ inputs.environment == 'prod' }}
    
  - id: storage
    name: S3 Storage Stack
    description: Creates S3 bucket with SQS notification for object uploads
    dir: stacks/storage/
    if: ${{ inputs.enable_storage_stack }}
    params:
      NotificationQueueArn: ${{ stacks.processor.outputs.NotificationQueueArn }}
      Keywords: ${{ stacks.processor.outputs.Keywords }}
      S3StorageClass: ${{ inputs.environment == 'prod' && 'STANDARD' || 'STANDARD_IA' }}
      EnableVersioning: ${{ inputs.environment == 'prod' }}
    run: |
      echo "S3 bucket deployed successfully to environment: ${{ inputs.environment }}"
      echo "Configuration details:"
      echo "  - Environment: ${{ inputs.environment }}"
      echo "  - Bucket Name: ${{ stacks.storage.outputs.BucketName }}"
      echo "  - Lambda Function: ${{ stacks.processor.outputs.ProcessorFunctionArn }}"
      
      # Test the setup by uploading a test file
      echo ""
      echo "Testing the pipeline by uploading a test file..."
      test_file="/tmp/test-file-${{ inputs.environment }}.txt"
      s3_path="s3://${{ stacks.storage.outputs.BucketName }}/test-${{ inputs.environment }}.txt"
      
      echo "Hello from samstacks in ${{ inputs.environment }}!" > "$test_file"
      
      if aws s3 cp "$test_file" "$s3_path"; then
        echo "Test file uploaded successfully"
        echo "Check CloudWatch logs for the Lambda function to see processing"
        
        # Clean up test file
        sleep 2
        aws s3 rm "$s3_path"
        rm -f "$test_file"
        echo "Test file cleaned up"
      else
        echo "Failed to upload test file"
        rm -f "$test_file"
        exit 1
      fi
      
      echo ""
      echo "Pipeline deployment and testing completed successfully!"

summary: |
  # S3 Object Processor - Deployment Complete!
  
  Your **${{ inputs.environment }}** environment has been successfully deployed with comprehensive security masking enabled.

  ## Deployment Configuration

  | Setting | Value |
  |---------|-------|
  | **Environment** | `${{ inputs.environment }}` |
  | **AWS Region** | `${{ pipeline.settings.default_region }}` |
  | **Storage Stack Enabled** | `${{ inputs.enable_storage_stack }}` |

  ## Infrastructure Deployed

  ### Processing Stack (`processor`)
  - **Lambda Function**: `${{ stacks.processor.outputs.ProcessorFunctionName }}`
  - **Function ARN**: `${{ stacks.processor.outputs.ProcessorFunctionArn }}`
  - **SQS Queue**: `${{ stacks.processor.outputs.NotificationQueueName }}`

  ### Storage Stack (`storage`)
  - **S3 Bucket**: `${{ stacks.storage.outputs.BucketName }}`
  - **Bucket ARN**: `${{ stacks.storage.outputs.BucketArn }}`
  - **Storage Class**: `${{ inputs.environment == 'prod' && 'STANDARD' || 'STANDARD_IA' }}`

  ## Security Features

  This deployment includes comprehensive output masking to protect sensitive data:
  - **Account IDs** - Masked in all ARNs and URLs
  - **API Endpoints** - API Gateway and Lambda URLs protected
  - **Database Endpoints** - Connection strings masked
  - **Load Balancer DNS** - ALB/NLB domains protected
  - **CloudFront Domains** - Distribution domains masked
  - **IP Addresses** - IPv4 and IPv6 addresses protected
  - **Custom Patterns** - Application-specific secrets masked

  > **Note**: All sensitive data in the above outputs has been automatically 
  > masked with asterisks (*) for security. This makes it safe to share 
  > deployment reports, logs, and artifacts without exposing sensitive information.

  ## What's Next?

  ### 1. **Test Your Pipeline**
  ```bash
  # Upload a test file to trigger processing
  aws s3 cp my-file.txt s3://${{ stacks.storage.outputs.BucketName || 'YOUR_BUCKET' }}/test-upload.txt
  ```

  ### 2. **Monitor Processing**
  - **CloudWatch Logs**: Monitor Lambda execution logs
  - **SQS Console**: Check queue metrics and dead letter queues
  - **S3 Console**: Verify object uploads and lifecycle policies

  ### 3. **Environment Management**
  Deploy to different environments:
  ```bash
  # Production deployment with security masking
  samstacks deploy examples/pipeline.yml \\
    --input environment=prod \\
    --report-file secure-deployment-report.md

  # Development deployment
  samstacks deploy examples/pipeline.yml \\
    --input environment=dev
  ```

  ---

  **Congratulations!** Your serverless object processing pipeline is ready in the **${{ inputs.environment }}** environment with comprehensive security protection! 